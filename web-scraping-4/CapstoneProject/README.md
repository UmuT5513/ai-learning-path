# Books to Scrape - Web Scraping Project

Bu proje, [Books to Scrape](https://books.toscrape.com/) web sitesinden "Food and Drink" ve "Travel" kategorilerindeki kitap bilgilerini otomatik olarak Ã§eken bir web scraping uygulamasÄ±dÄ±r.

## ğŸ“‹ Proje Ã–zeti

Proje, Selenium WebDriver ve BeautifulSoup kullanarak kitap bilgilerini Ã§eker ve CSV formatÄ±nda saklar. Uygulama, sayfalama (pagination) desteÄŸi ile tÃ¼m kategorilerdeki kitaplarÄ± otomatik olarak tarar.

## ğŸš€ Ã–zellikler

- **Kategori BazlÄ± Tarama**: Food and Drink ve Travel kategorilerini otomatik olarak tarar
- **Sayfalama DesteÄŸi**: Her kategorinin tÃ¼m sayfalarÄ±nÄ± otomatik olarak gezinir
- **DetaylÄ± Bilgi Ã‡ekme**: Her kitap iÃ§in ayrÄ±ntÄ±lÄ± bilgileri toplar
- **Hata YÃ¶netimi**: Eksik veriler iÃ§in gÃ¼venli hata yÃ¶netimi
- **CSV Ã‡Ä±ktÄ±sÄ±**: Toplanan verileri CSV formatÄ±nda kaydeder

## ğŸ“Š Toplanan Veriler

Her kitap iÃ§in aÅŸaÄŸÄ±daki bilgiler toplanÄ±r:

- **Kitap AdÄ±** (Name)
- **Fiyat** (Price)
- **AÃ§Ä±klama** (Description)
- **Vergi** (Tax)
- **Vergisiz Fiyat** (Price with Tax)
- **Stok Durumu** (Availability)
- **Yorum SayÄ±sÄ±** (Number of Reviews)
- **UPC Kodu** (UPC)

## ğŸ› ï¸ Teknolojiler

- **Python 3.x**
- **Selenium WebDriver** - Web tarayÄ±cÄ± otomasyonu
- **BeautifulSoup** - HTML parsing
- **lxml** - XML/HTML processing
- **pandas** - Veri manipÃ¼lasyonu ve CSV Ã§Ä±ktÄ±sÄ±
- **Chrome WebDriver** - TarayÄ±cÄ± kontrolÃ¼

## ğŸ“¦ Kurulum

### Gereksinimler

```bash
pip install selenium beautifulsoup4 lxml pandas
```

### Chrome WebDriver

1. Chrome tarayÄ±cÄ±nÄ±zÄ±n sÃ¼rÃ¼mÃ¼nÃ¼ kontrol edin
2. [ChromeDriver](https://chromedriver.chromium.org/) sayfasÄ±ndan uygun sÃ¼rÃ¼mÃ¼ indirin
3. ChromeDriver'Ä± PATH'e ekleyin veya proje dizinine yerleÅŸtirin

## ğŸ¯ KullanÄ±m

```bash
python scraper.py
```

### Kod Ã‡alÄ±ÅŸtÄ±rma AdÄ±mlarÄ±

1. Uygulama Chrome tarayÄ±cÄ±sÄ±nÄ± aÃ§ar
2. Books to Scrape sitesine gider
3. "Food and Drink" ve "Travel" kategorilerini bulur
4. Her kategoriyi sÄ±rayla iÅŸler:
   - Kategoriye tÄ±klar
   - Sayfadaki tÃ¼m kitaplarÄ± bulur
   - Her kitabÄ±n detay sayfasÄ±na gider
   - Bilgileri Ã§eker ve listede saklar
   - Sonraki sayfaya geÃ§er (varsa)
5. TÃ¼m veriler CSV dosyasÄ±na kaydedilir

## ğŸ“ Ã‡Ä±ktÄ± DosyalarÄ±

- `outputs/miuul_capstone.csv` - Birincil Ã§Ä±ktÄ± dosyasÄ±
- `C:/Users/Umut/Desktop/ai-learning-path/web-scraping-4/outputs/miuul_webscraping_capstone.csv` - Yedek Ã§Ä±ktÄ± dosyasÄ±

## âš™ï¸ KonfigÃ¼rasyon

### Chrome SeÃ§enekleri

```python
options = webdriver.ChromeOptions()
options.add_argument("--start-minimized")  # KÃ¼Ã§Ã¼k ekran modunda baÅŸlat
# options.add_argument("--headless")        # GÃ¶rÃ¼nmez mod (isteÄŸe baÄŸlÄ±)
```

### Bekleme SÃ¼releri

- Sayfa yÃ¼kleme: 3 saniye
- Kategori geÃ§iÅŸi: 2 saniye
- Kitap detayÄ±: 2 saniye
- Geri dÃ¶nÃ¼ÅŸ: 2 saniye

## ğŸ”§ Hata YÃ¶netimi

- **StaleElementReferenceException**: WebDriverWait ile Ã§Ã¶zÃ¼lÃ¼r
- **Eksik Veri**: BoÅŸ string ile doldurulur
- **Liste Uzunluk UyumsuzluÄŸu**: Otomatik dÃ¼zeltme
- **CSV Kaydetme HatasÄ±**: Alternatif dosya yolu

## ğŸ“ˆ Performans

- Ã‡alÄ±ÅŸma sÃ¼resini Ã¶lÃ§er ve gÃ¶sterir
- Sayfa geÃ§iÅŸlerini loglar
- Ä°ÅŸlem tamamlandÄ±ÄŸÄ±nda bildirim verir

## ğŸš¨ Dikkat Edilmesi Gerekenler

1. **Robot.txt**: Web sitesinin robot.txt kurallarÄ±na uyulmalÄ±
2. **Rate Limiting**: Ã‡ok hÄ±zlÄ± istek gÃ¶ndermekten kaÃ§Ä±nÄ±n
3. **Etik Scraping**: Sitenin kaynaklarÄ±nÄ± aÅŸÄ±rÄ± yÃ¼klemeyin
4. **Yasal Sorumluluk**: KullanÄ±m ÅŸartlarÄ±nÄ± kontrol edin

## ğŸ› Bilinen Sorunlar

- BazÄ± kitaplarda eksik bilgiler olabilir
- AÄŸ baÄŸlantÄ± sorunlarÄ± scraping'i kesebilir
- Chrome gÃ¼ncellemeleri WebDriver uyumsuzluÄŸuna neden olabilir

## ğŸ”„ Gelecek GeliÅŸtirmeler

- [ ] Daha fazla kategori desteÄŸi
- [ ] Paralel iÅŸleme
- [ ] VeritabanÄ± entegrasyonu
- [ ] API endpoint'i
- [ ] Daha geliÅŸmiÅŸ hata yÃ¶netimi

## ğŸ“„ Lisans

Bu proje eÄŸitim amaÃ§lÄ± olarak geliÅŸtirilmiÅŸtir.

## ğŸ¤ KatkÄ±da Bulunma

1. Fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/YeniOzellik`)
3. Commit edin (`git commit -am 'Yeni Ã¶zellik eklendi'`)
4. Branch'i push edin (`git push origin feature/YeniOzellik`)
5. Pull Request oluÅŸturun

## ğŸ“ Ä°letiÅŸim

SorularÄ±nÄ±z iÃ§in GitHub Issues kullanabilirsiniz.

---

**Not**: Bu proje sadece eÄŸitim amaÃ§lÄ±dÄ±r. Web scraping yaparken her zaman sitenin kullanÄ±m ÅŸartlarÄ±na ve etik kurallara uyunuz.